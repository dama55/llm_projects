services:
  # 1) API入口（軽い）：SSE/UI/認証/ログ/バックエンド切替など
  gateway:
    build:
      context: ./gateway          # ← 旧: context: . （ルート）から変更
      dockerfile: Dockerfile
    container_name: gateway
    ports:
      - "8000:8000"               # ← 旧 llm が開けていた 8000 を gateway が担当
    environment:
      - PYTHONUNBUFFERED=1
      - VLLM_BASE_URL=http://vllm:8000     # ← gateway -> vLLM の中継先
      - DEFAULT_MODEL=Qwen/Qwen2.5-7B-Instruct
      # - APP_ENV=${APP_ENV}               # 必要なら残す（推論には不要）
    volumes:
      - ./gateway:/app            # (任意) 開発中にホットリロードしたい場合だけ
    depends_on:
      - vllm

  # 2) 推論エンジン（重い）：OpenAI互換APIで待ち受ける
  vllm:
    image: vllm/vllm-openai:latest
    container_name: vllm
    entrypoint: ["vllm", "serve"]
    command: >
      rinna/qwen2.5-bakeneko-32b-instruct-awq
      --quantization awq_marlin
      --dtype float16
      --host 0.0.0.0
      --port 8000
      --max-model-len 7168
      --gpu-memory-utilization 0.90
      --max-num-batched-tokens 1024
    # 32Bモデルで4Bit量子化（AWQ）版
    # --model Qwen/Qwen2.5-32B-Instruct-AWQ
    # 軽いモデル=>かなり高速で生成できた．結構内容も良い．
    # --model Qwen/Qwen2.5-7B-Instruct
    # gateway経由だけなら、外部に公開しなくてもOK（安全）
    ports:
      - "8001:8000"             # 直接叩いて動作確認したいなら開ける

    environment:
      # HFのPrivateモデルやレート制限回避で必要なら
      - HUGGING_FACE_HUB_TOKEN=${HUGGINGFACE_TOKEN}

    volumes:
      # HFモデルを毎回落とさないためのキャッシュ（おすすめ）
      - huggingface:/root/.cache/huggingface
      # vLLMが独自に使うキャッシュが必要なら（任意）
      - vllm_cache:/root/.cache/vllm

    # GPUをvLLMにだけ渡す（gatewayには不要）
    # まずはこれを使うのが一番シンプル（対応していれば）
    gpus: all

    # もし gpus: all が効かない環境なら、代替として device_requests を使う
    # device_requests:
    #   - driver: nvidia
    #     count: 1
    #     capabilities: ["gpu"]

volumes:
  huggingface:
  vllm_cache: