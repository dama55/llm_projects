qwenの2.5からthinking推論可能な3モデルに変更する．
3モデルの日本語特化版は今のところないらしい．

もともとの設定
```
rinna/qwen2.5-bakeneko-32b-instruct-awq
--quantization awq_marlin
--dtype float16
--host 0.0.0.0
--port 8000
--max-model-len 7040
--gpu-memory-utilization 0.90
--max-num-batched-tokens 1024
```

変更後

出力がめちゃめちゃ遅くなったので，できる限り設定で軽くできないかためした．

```
Qwen/Qwen3-32B-AWQ
--quantization awq
--dtype float16
--host 0.0.0.0
--port 8000
--max-model-len 4096
--gpu-memory-utilization 0.92
--max-num-batched-tokens 512
```

クライアント側から，以下のように`chat_template_kwargs: { enable_thinking: false }`で考えることをやめさせた理もしたが，大して速度は改善しなかった．

```
// OpenAI互換のリクエストJSON（gateway側でmodel補完やsystem挿入をするので最小でもOK）
  const body = {
	stream: true, // ストリームで返してもらう
	messages: [
	  { role: "user", content: prompt }
	],
	temperature: 0.7,
	top_p: 0.9,
	max_tokens: 2500,
	// ★追加：Qwen3のthinking切替（vLLMのReasoning Outputs機能）
	chat_template_kwargs: { enable_thinking: false },
  };
```

異様に遅かったので，元に戻したら速度が遅いままだった！
おかしいなと思ったら，`--quantization awq_marlin`こいつを戻し忘れていることに気づいた．

>[!note]
>awq_marlin は何を指定してる？
>- AWQ：4bit量子化の方式（重みを4bitにして軽くする）
>- Marlin：NVIDIA GPU向けの高速4bit行列積カーネル（主にAda世代/4090と相性良い）
>- vLLMの --quantization awq_marlin は**「AWQ重みを読み込みつつ、計算はMarlinでやる」**という指定です。なので --quantization awq と比べると、だいたい：
>- ✅ 生成が速くなりやすい
>- ✅ 場合によって ワークスペース/一時領域の使い方が変わる
>- ⚠️ モデルやGPUやvLLM版によって 対応してないと落ちる/遅くなることもある
>
>`--quantization awq` の場合、AWQ重み（4bit）でも
> 汎用寄りの実装で4bitを扱う（バックエンドがそこまで最適じゃない）
> 
> 演算時のデコード/スケール適用/メモリアクセスが効率悪いことがある
> 
> 一方 `--quantization awq_marlin` は
> 
> Marlinという4bit GEMM専用カーネルで回す
> 
> GPUのTensor Coreやメモリアクセスを前提に最適化されていて、特に RTX 4090（Ada） と相性が良い
> 
> 結果として：
> 
> tokens/sec が伸びる
> 
> 最初の出力までの時間（TTFT）も縮むことが多い



marlinを使うことで超爆速になった．
そこで，marlinを使った構成に変更する

```
Qwen/Qwen3-32B-AWQ
--quantization awq_marlin
--dtype float16
--host 0.0.0.0
--port 8000
--max-model-len 4096
--gpu-memory-utilization 0.92
--max-num-batched-tokens 512
```

結果的に思考する場合でもかなり素早く出力できることが分かった．

![[Pasted image 20260217041516.png]]
## 学び

- `--quantization awq_marlin`が速度に超大事．
- qwen3のthinkingバージョンでもかなりの速度で生成できることが分かった．